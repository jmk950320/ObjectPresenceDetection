"""
ResNet, MobileNet, VGG ë“±ì—ì„œ íŠ¹ì§• ì¶”ì¶œí•˜ëŠ” ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤

ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨:
    with torch.inference_mode():
        with torch.autocast(device_type='cuda', dtype=torch.float32):
            feats = model.get_intermediate_layers(image_resized_norm.unsqueeze(0).cuda(), n=range(n_layers), reshape=True, norm=True)
            x = feats[-1].squeeze().detach().cpu()
            dim = x.shape[0]
            x = x.view(dim, -1).permute(1, 0)  # feature cnt, dims
"""

import sys
import os
from pathlib import Path

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as transforms
from typing import List, Optional, Union, Tuple
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from feature_extractor import FeatureExtractor
from utils.read_video import read_video

class UnifiedCNNModel:
    """
    ResNet, MobileNet, VGG ë“±ì„ í†µí•©í•œ ëª¨ë¸ ë˜í¼
    ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
    """
    
    def __init__(self, model_name: str = 'resnet50', pretrained: bool = True, device: str = 'cuda'):
        """
        ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            model_name (str): ëª¨ë¸ ì´ë¦„ ('resnet50', 'resnet101', 'mobilenet_v2', 'vgg16', 'vgg19')
            pretrained (bool): ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
            device (str): ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cpu')
        """
        self.extractor = FeatureExtractor(model_name, pretrained, device)
        self.model_name = model_name
        self.device = device
    
    def get_intermediate_layers(
        self, 
        image: torch.Tensor, 
        n: Optional[Union[List[int], range]] = None,
        reshape: bool = True,
        norm: bool = True
    ) -> List[torch.Tensor]:
        """
        ì¤‘ê°„ ë ˆì´ì–´ íŠ¹ì§• ì¶”ì¶œ (ê¸°ì¡´ ì¸í„°í˜ì´ìŠ¤ì™€ í˜¸í™˜)
        
        Args:
            image (torch.Tensor): ì…ë ¥ ì´ë¯¸ì§€ (B, C, H, W)
            n (Optional[Union[List[int], range]]): ì¶”ì¶œí•  ë ˆì´ì–´ ì¸ë±ìŠ¤
            reshape (bool): íŠ¹ì§•ì„ reshape í• ì§€ ì—¬ë¶€
            norm (bool): ì •ê·œí™” ì—¬ë¶€
        
        Returns:
            List[torch.Tensor]: ì¶”ì¶œëœ íŠ¹ì§• ë¦¬ìŠ¤íŠ¸
        """
        # nì´ range ê°ì²´ì¸ ê²½ìš° ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(n, range):
            layer_indices = list(n)
        elif n is None:
            layer_indices = None
        else:
            layer_indices = n
        
        # íŠ¹ì§• ì¶”ì¶œ
        features = self.extractor.extract_features(
            image, 
            layer_indices=layer_indices,
            normalize=norm,
            reshape=reshape
        )
        
        return features
    
    def cuda(self):
        """CUDAë¡œ ì´ë™ (í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ)"""
        self.extractor.device = 'cuda'
        self.extractor.model = self.extractor.model.cuda()
        return self
    
    def cpu(self):
        """CPUë¡œ ì´ë™ (í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ)"""
        self.extractor.device = 'cpu'
        self.extractor.model = self.extractor.model.cpu()
        return self
    
    def eval(self):
        """í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (í˜¸í™˜ì„±ì„ ìœ„í•œ ë©”ì„œë“œ)"""
        self.extractor.model.eval()
        return self


def create_model(model_name: str = 'resnet50', pretrained: bool = True, device: str = 'cuda') -> UnifiedCNNModel:
    """
    í†µí•© ëª¨ë¸ ìƒì„± í•¨ìˆ˜
    
    Args:
        model_name (str): ëª¨ë¸ ì´ë¦„
        pretrained (bool): ì‚¬ì „ í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì‚¬ìš© ì—¬ë¶€
        device (str): ë””ë°”ì´ìŠ¤
    
    Returns:
        UnifiedCNNModel: í†µí•© ëª¨ë¸ ê°ì²´
    """
    return UnifiedCNNModel(model_name, pretrained, device)


def extract_features_like_original(
    image_resized_norm: torch.Tensor,
    model_name: str = 'resnet50',
    n_layers: Union[int, List[int]] = 4,
    device: str = 'cuda',
    model: Optional[UnifiedCNNModel] = None
) -> torch.Tensor:
    """
    ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
    
    Args:
        image_resized_norm (torch.Tensor): ì •ê·œí™”ëœ ì´ë¯¸ì§€ (C, H, W)
        model_name (str): ëª¨ë¸ ì´ë¦„
        n_layers (Union[int, List[int]]): ì¶”ì¶œí•  ë ˆì´ì–´ ìˆ˜ ë˜ëŠ” íŠ¹ì • ë ˆì´ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
                                          ì˜ˆ: 4 -> [0,1,2,3], [2,3] -> 2ë²ˆê³¼ 3ë²ˆ ë ˆì´ì–´ë§Œ
        device (str): ë””ë°”ì´ìŠ¤
        model (Optional[UnifiedCNNModel]): ë¯¸ë¦¬ ë¡œë“œëœ ëª¨ë¸ ê°ì²´ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
    
    Returns:
        torch.Tensor: ì¶”ì¶œëœ íŠ¹ì§• (feature_cnt, dims)
    """
    # ëª¨ë¸ ìƒì„± (ì „ë‹¬ë°›ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ)
    if model is None:
        model = create_model(model_name, pretrained=True, device=device)
    
    # ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
    with torch.inference_mode():
        with torch.autocast(device_type=device, dtype=torch.float32):
            # n_layersê°€ intë©´ rangeë¡œ, listë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            layer_spec = [n_layers] if isinstance(n_layers, int) else n_layers
            
            feats = model.get_intermediate_layers(
                image_resized_norm.unsqueeze(0).to(device), 
                n=layer_spec, 
                reshape=True, 
                norm=True
            )
            
            # ë§ˆì§€ë§‰ ë ˆì´ì–´ íŠ¹ì§• ì‚¬ìš©
            x = feats[-1].squeeze().detach().cpu()
            
            # Spatial dimensions ì €ì¥ (C, H, W) -> (H, W)
            # ResNetì˜ ê²½ìš° ë³´í†µ (2048, 7, 7) í˜•íƒœì„
            if len(x.shape) == 3:
                h_patches, w_patches = x.shape[1], x.shape[2]
            else:
                # 1D featureì¸ ê²½ìš° (Global Average Pooling ì´í›„ ë“±)
                h_patches, w_patches = 1, 1
            
            dim = x.shape[0]
            
            # ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼í•œ reshape
            x = x.view(dim, -1).permute(1, 0)  # feature cnt, dims
    
    return x, h_patches, w_patches


def visualize_features_pca(features: torch.Tensor, h_patches: int, w_patches: int, original_image: Optional[torch.Tensor] = None, save_path: Optional[str] = None, n_components: int = 3):
    """
    PCAë¥¼ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§•ì„ ì‹œê°í™”
    
    Args:
        features (torch.Tensor): ì¶”ì¶œëœ íŠ¹ì§• (feature_cnt, dims)
        h_patches (int): ë†’ì´ íŒ¨ì¹˜ ìˆ˜
        w_patches (int): ë„ˆë¹„ íŒ¨ì¹˜ ìˆ˜
        original_image (Optional[torch.Tensor]): ì›ë³¸ ì´ë¯¸ì§€ (C, H, W)
        save_path (Optional[str]): ì €ì¥ ê²½ë¡œ
        n_components (int): PCA ì°¨ì› ìˆ˜ (1, 2, 3)
    """
    # PCA ì ìš©
    pca = PCA(n_components=n_components, whiten=True)
    x_np = features.numpy()
    pca.fit(x_np)
    
    # PCA ë³€í™˜
    projected_features = pca.transform(x_np)
    
    # ì°¨ì›ì— ë”°ë¥¸ ì‹œê°í™” ì²˜ë¦¬
    if n_components == 1:
        # 1ì°¨ì›: Min-Max ì •ê·œí™” í›„ Jet Colormap ì ìš©
        proj_min, proj_max = projected_features.min(), projected_features.max()
        norm_features = (projected_features - proj_min) / (proj_max - proj_min + 1e-6)
        
        # (H, W) í˜•íƒœë¡œ ë³€í™˜
        heatmap = norm_features.reshape(h_patches, w_patches)
        
        # Colormap ì ìš© (numpy array ë°˜í™˜ë¨: H, W, 4)
        cmap = plt.get_cmap('jet')
        colored_map = cmap(heatmap)
        
        # RGBë§Œ ì‚¬ìš© (H, W, 3) -> (3, H, W)
        projected_image = torch.from_numpy(colored_map[:, :, :3]).permute(2, 0, 1).float()
        
    elif n_components == 2:
        # 2ì°¨ì›: R, G ì±„ë„ì— ë§¤í•‘, BëŠ” 0
        projected_features_tensor = torch.from_numpy(projected_features).view(h_patches, w_patches, 2)
        
        # Sigmoidë¡œ ìƒ‰ìƒ ê°•í™” (ê¸°ì¡´ ë¡œì§ í™œìš©)
        projected_features_tensor = torch.nn.functional.sigmoid(projected_features_tensor.mul(2.0))
        
        # Blue ì±„ë„ (0) ì¶”ê°€
        zeros = torch.zeros(h_patches, w_patches, 1)
        projected_image = torch.cat([projected_features_tensor, zeros], dim=2).permute(2, 0, 1)
        
    else: # n_components == 3
        # 3ì°¨ì›: ê¸°ì¡´ ë¡œì§ (RGB)
        projected_image = torch.from_numpy(projected_features).view(h_patches, w_patches, 3)
        projected_image = torch.nn.functional.sigmoid(projected_image.mul(2.0)).permute(2, 0, 1)
    
    # ì‹œê°í™”
    plt.figure(figsize=(20, 8), dpi=300)
    
    # ì›ë³¸ ì´ë¯¸ì§€ í‘œì‹œ (ìˆì„ ê²½ìš°)
    if original_image is not None:
        plt.subplot(1, 2, 1)
        # Tensor (C, H, W) -> Numpy (H, W, C)
        img_np = original_image.permute(1, 2, 0).cpu().numpy()
        # Denormalize if needed (assuming ImageNet normalization)
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img_np = std * img_np + mean
        img_np = np.clip(img_np, 0, 1)
        
        plt.imshow(img_np)
        plt.title("Original Image")
        plt.axis('off')
        
        plt.subplot(1, 2, 2)
    
    # PCA ê²°ê³¼ ì´ë¯¸ì§€ í‘œì‹œ
    # Upsample for better visualization if patches are small
    if h_patches < 224:
        projected_image_up = torch.nn.functional.interpolate(
            projected_image.unsqueeze(0), 
            size=(224, 224), 
            mode='bilinear', 
            align_corners=False
        ).squeeze(0)
    else:
        projected_image_up = projected_image
        
    plt.imshow(projected_image_up.permute(1, 2, 0).numpy())
    plt.title(f"PCA Visualization (dims: {features.shape[1]} -> {n_components})")
    plt.axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, bbox_inches='tight', dpi=300)
        print(f"ê²°ê³¼ ì €ì¥ë¨: {save_path}")
    
    # plt.show() # ì„œë²„ í™˜ê²½ì—ì„œëŠ” ì£¼ì„ ì²˜ë¦¬
    plt.close() # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€


# ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì§• ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def extract_features_from_video(
    video_path: str,
    model_name: str = 'resnet50',
    n_layers: Union[int, List[int]] = 4,
    mask_path: Optional[str] = None,
    roi: Optional[Tuple[int, int, int, int]] = None,
    roi_format: str = 'xywh',
    start_frame: int = 0,
    end_frame: Optional[int] = None,
    device: str = 'cuda',
    output_dir: Optional[str] = None,
    pca_dim: int = 3
) -> List[torch.Tensor]:
    """
    ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ë³„ë¡œ íŠ¹ì§• ì¶”ì¶œ (4ê°€ì§€ ì „ì²˜ë¦¬ ê²½ìš° ì§€ì›)
    
    Args:
        video_path (str): ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        model_name (str): ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
        n_layers (Union[int, List[int]]): ì¶”ì¶œí•  ë ˆì´ì–´ ìˆ˜ ë˜ëŠ” íŠ¹ì • ë ˆì´ì–´ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        mask_path (Optional[str]): ë§ˆìŠ¤í¬ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)
        roi (Optional[Tuple[int, int, int, int]]): ROI ì¢Œí‘œ (ì„ íƒì‚¬í•­)
        roi_format (str): ROI í˜•ì‹ ('xywh' ë˜ëŠ” 'xyxy')
        start_frame (int): ì‹œì‘ í”„ë ˆì„ ë²ˆí˜¸
        end_frame (Optional[int]): ì¢…ë£Œ í”„ë ˆì„ ë²ˆí˜¸
        device (str): ë””ë°”ì´ìŠ¤
        output_dir (Optional[str]): íŠ¹ì§•ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬ (ì„ íƒì‚¬í•­)
    
    Returns:
        List[torch.Tensor]: í”„ë ˆì„ë³„ ì¶”ì¶œëœ íŠ¹ì§• ë¦¬ìŠ¤íŠ¸
    """
    import sys
    import os
    from pathlib import Path
    
    # utils ëª¨ë“ˆ import
    sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
    from utils.read_video import VideoFrameReader, VideoMaskProcessor, VideoROIProcessor, VideoMaskROIProcessor
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: video_name/model_name/layer_number
    if output_dir:
        # ë¹„ë””ì˜¤ ì´ë¦„ ì¶”ì¶œ (í™•ì¥ì ì œì™¸)
        video_name = Path(video_path).stem
        
        # ë ˆì´ì–´ ë²ˆí˜¸ ê²°ì •
        if isinstance(n_layers, int):
            # n_layersê°€ intë©´ ë§ˆì§€ë§‰ ë ˆì´ì–´ ë²ˆí˜¸ ì‚¬ìš©
            layer_number = n_layers - 1
        elif isinstance(n_layers, list):
            # listë©´ ë§ˆì§€ë§‰ ë ˆì´ì–´ ì¸ë±ìŠ¤ ì‚¬ìš©
            layer_number = n_layers[-1]
        else:
            layer_number = 0
        
        # ìµœì¢… ì¶œë ¥ ê²½ë¡œ: output_dir/video_name/model_name/mode/layer_number/pca{dim}
        if mask_path and roi:
            mode_dir = "mask_roi"
        elif mask_path:
            mode_dir = "mask"
        elif roi:
            mode_dir = "roi"
        else:
            mode_dir = "normal" # ë˜ëŠ” ë¹ˆ ë¬¸ìì—´, í•˜ì§€ë§Œ êµ¬ì¡° í†µì¼ì„±ì„ ìœ„í•´ ëª…ì‹œ
            
        # normal ëª¨ë“œì¼ ê²½ìš° ê²½ë¡œë¥¼ ì¤„ì¼ì§€ ì—¬ë¶€ëŠ” ì„ íƒì‚¬í•­ì´ì§€ë§Œ, 
        # ì‚¬ìš©ì ìš”ì²­ ì˜ˆì‹œ(.../roi/1/pca3)ì— ë§ì¶”ë ¤ë©´ mode ë””ë ‰í† ë¦¬ê°€ ìˆëŠ” ê²ƒì´ ì¢‹ìŒ
        if mode_dir == "normal":
             final_output_dir = Path(output_dir) / video_name / model_name / str(layer_number) / f"pca{pca_dim}"
        else:
             final_output_dir = Path(output_dir) / video_name / model_name / mode_dir / str(layer_number) / f"pca{pca_dim}"
             
        final_output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {final_output_dir}")
    else:
        final_output_dir = None
    
    # ëª¨ë¸ ìƒì„±
    model = create_model(model_name, pretrained=True, device=device)
    
    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë³€í™˜
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    
    all_features = []
    
    # 4ê°€ì§€ ê²½ìš°ì— ë”°ë¼ ì²˜ë¦¬
    # Case 1: ë§ˆìŠ¤í¬ + ROI
    if mask_path and roi:
        print(f"ğŸ¬ ë¹„ë””ì˜¤ + ë§ˆìŠ¤í¬ + ROI ì²˜ë¦¬: {video_path}")
        print(f"   ë§ˆìŠ¤í¬: {mask_path}")
        print(f"   ROI: {roi} (í˜•ì‹: {roi_format})")
        
        with VideoMaskROIProcessor(video_path, mask_path, roi, roi_format) as processor:
            video_info = processor.get_video_info()
            print(f"ë¹„ë””ì˜¤ ì •ë³´: {video_info['frame_count']}í”„ë ˆì„, {video_info['width']}x{video_info['height']}")
            print(f"ROI ì •ë³´: {video_info['roi']}")
            
            for frame_num, original_frame, processed_frame in processor.process_frames_with_mask_and_roi(start_frame, end_frame):
                # ì „ì²˜ë¦¬ëœ í”„ë ˆì„(ë§ˆìŠ¤í¬+ROI) ì‚¬ìš©
                frame_tensor = transform(processed_frame)
                
                # íŠ¹ì§• ì¶”ì¶œ (ëª¨ë¸ ì¬ì‚¬ìš©)
                features, h, w = extract_features_like_original(frame_tensor, model_name, n_layers, device, model=model)
                all_features.append(features)
                
                print(f"  í”„ë ˆì„ {frame_num}: íŠ¹ì§• shape {features.shape}, Grid: {h}x{w}")
                
                # ì‹œê°í™” ë° ì €ì¥ (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì‚¬ìš©)
                if final_output_dir:
                    vis_path = final_output_dir / f"frame_{frame_num:04d}_pca.png"
                    visualize_features_pca(features, h, w, frame_tensor, str(vis_path), n_components=pca_dim)
    
    # Case 2: ë§ˆìŠ¤í¬ë§Œ
    elif mask_path:
        print(f"ğŸ¬ ë¹„ë””ì˜¤ + ë§ˆìŠ¤í¬ ì²˜ë¦¬: {video_path}")
        print(f"   ë§ˆìŠ¤í¬: {mask_path}")
        
        with VideoMaskProcessor(video_path, mask_path) as processor:
            video_info = processor.get_video_info()
            print(f"ë¹„ë””ì˜¤ ì •ë³´: {video_info['frame_count']}í”„ë ˆì„, {video_info['width']}x{video_info['height']}")
            
            for frame_num, original_frame, processed_frame in processor.process_frames_with_mask(start_frame, end_frame):
                # ì „ì²˜ë¦¬ëœ í”„ë ˆì„(ë§ˆìŠ¤í¬) ì‚¬ìš©
                frame_tensor = transform(processed_frame)
                
                # íŠ¹ì§• ì¶”ì¶œ (ëª¨ë¸ ì¬ì‚¬ìš©)
                features, h, w = extract_features_like_original(frame_tensor, model_name, n_layers, device, model=model)
                all_features.append(features)
                
                print(f"  í”„ë ˆì„ {frame_num}: íŠ¹ì§• shape {features.shape}, Grid: {h}x{w}")
                
                # ì‹œê°í™” ë° ì €ì¥ (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì‚¬ìš©)
                if final_output_dir:
                    vis_path = final_output_dir / f"frame_{frame_num:04d}_pca.png"
                    visualize_features_pca(features, h, w, frame_tensor, str(vis_path), n_components=pca_dim)
    
    # Case 3: ROIë§Œ
    elif roi:
        print(f"ğŸ¬ ë¹„ë””ì˜¤ + ROI ì²˜ë¦¬: {video_path}")
        print(f"   ROI: {roi} (í˜•ì‹: {roi_format})")
        
        with VideoROIProcessor(video_path, roi, roi_format) as processor:
            video_info = processor.get_video_info()
            print(f"ë¹„ë””ì˜¤ ì •ë³´: {video_info['frame_count']}í”„ë ˆì„, {video_info['width']}x{video_info['height']}")
            print(f"ROI ì •ë³´: {video_info['roi']}")
            
            for frame_num, original_frame, processed_frame in processor.process_frames_with_roi(start_frame, end_frame):
                # ì „ì²˜ë¦¬ëœ í”„ë ˆì„(ROI) ì‚¬ìš©
                frame_tensor = transform(processed_frame)
                
                # íŠ¹ì§• ì¶”ì¶œ (ëª¨ë¸ ì¬ì‚¬ìš©)
                features, h, w = extract_features_like_original(frame_tensor, model_name, n_layers, device, model=model)
                all_features.append(features)
                
                print(f"  í”„ë ˆì„ {frame_num}: íŠ¹ì§• shape {features.shape}, Grid: {h}x{w}")
                
                # ì‹œê°í™” ë° ì €ì¥ (ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì‚¬ìš©)
                if final_output_dir:
                    vis_path = final_output_dir /f"frame_{frame_num:04d}_pca.png"
                    visualize_features_pca(features, h, w, frame_tensor, str(vis_path), n_components=pca_dim)
    
    # Case 4: ê¸°ë³¸ (ì „ì²˜ë¦¬ ì—†ìŒ)
    else:
        print(f"ğŸ¬ ë¹„ë””ì˜¤ ì²˜ë¦¬ (ì „ì²˜ë¦¬ ì—†ìŒ): {video_path}")
        # read_video ì œë„ˆë ˆì´í„° ì‚¬ìš©
        from utils.read_video import read_video
        
        # ë¹„ë””ì˜¤ ì •ë³´ëŠ” cv2ë¡œ ì§ì ‘ í™•ì¸
        import cv2
        cap = cv2.VideoCapture(video_path)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        cap.release()
        
        print(f"ë¹„ë””ì˜¤ ì •ë³´: {frame_count}í”„ë ˆì„, {width}x{height}")
        
        for frame_num, frame in enumerate(read_video(video_path)):
            # ë²”ìœ„ ì²´í¬
            if frame_num < start_frame:
                continue
            if end_frame is not None and frame_num >= end_frame:
                break
                
            # í”„ë ˆì„ ì „ì²˜ë¦¬
            frame_tensor = transform(frame)
            
            # íŠ¹ì§• ì¶”ì¶œ (ëª¨ë¸ ì¬ì‚¬ìš©)
            features, h, w = extract_features_like_original(frame_tensor, model_name, n_layers, device, model=model)
            all_features.append(features)
            
            print(f"  í”„ë ˆì„ {frame_num}: íŠ¹ì§• shape {features.shape}, Grid: {h}x{w}")
            
            # ì‹œê°í™” ë° ì €ì¥
            if final_output_dir:
                vis_path = final_output_dir  / f"frame_{frame_num:04d}_pca.png"
                visualize_features_pca(features, h, w, frame_tensor, str(vis_path))
    
    print(f"\nâœ… ì´ {len(all_features)}ê°œ í”„ë ˆì„ ì²˜ë¦¬ ì™„ë£Œ")
    return all_features



# ë‹¤ì–‘í•œ ëª¨ë¸ì—ì„œ íŠ¹ì§• ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def compare_model_features(image: torch.Tensor, models_to_test: List[str] = None) -> dict:
    """
    ì—¬ëŸ¬ ëª¨ë¸ì—ì„œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ë¹„êµ
    
    Args:
        image (torch.Tensor): ì…ë ¥ ì´ë¯¸ì§€ (C, H, W)
        models_to_test (List[str]): í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        dict: ëª¨ë¸ë³„ íŠ¹ì§• ì •ë³´
    """
    if models_to_test is None:
        models_to_test = ['resnet50', 'resnet101', 'mobilenet_v2', 'vgg16', 'vgg19']
    
    results = {}
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    for model_name in models_to_test:
        try:
            print(f"Testing {model_name}...")
            features, h, w = extract_features_like_original(image, model_name, n_layers=4, device=device)
            
            results[model_name] = {
                'shape': features.shape,
                'feature_count': features.shape[0],
                'feature_dims': features.shape[1],
                'mean': features.mean().item(),
                'std': features.std().item()
            }
            
            print(f"  {model_name}: {features.shape} (feature_cnt, dims)")
            
        except Exception as e:
            print(f"  {model_name}: Error - {e}")
            results[model_name] = {'error': str(e)}
    
    return results


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='ë¹„ë””ì˜¤ì—ì„œ CNN íŠ¹ì§• ì¶”ì¶œ')
    parser.add_argument('--video', type=str, default='/home/kjm/foreground_segmentation/dataset/video/all normal 2.avi', \
                                                                                            help='ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--mask', type=str, default=None, help='ë§ˆìŠ¤í¬ íŒŒì¼ ê²½ë¡œ (ì„ íƒì‚¬í•­)')
    parser.add_argument('--roi', type=int, nargs=4, default=None, metavar=('X', 'Y', 'W', 'H'),
                       help='ROI ì¢Œí‘œ (ì˜ˆ: --roi 100 100 300 300)')
    parser.add_argument('--roi-format', type=str, default='xywh', choices=['xywh', 'xyxy'],
                       help='ROI í˜•ì‹ (xywh: x,y,width,height ë˜ëŠ” xyxy: x1,y1,x2,y2)')
    parser.add_argument('--model', type=str, default='resnet50', 
                       choices=['resnet18', 'resnet34', 'resnet50', 'resnet101', 
                               'mobilenet_v2', 'mobilenet_v3_small', 'mobilenet_v3_large',
                               'vgg16', 'vgg19', 'vgg16_bn', 'vgg19_bn'],
                       help='ì‚¬ìš©í•  ëª¨ë¸')
    parser.add_argument('--layers', type=int, default=4, help='ì¶”ì¶œí•  ë ˆì´ì–´ ìˆ˜ (--layer-indicesì™€ í•¨ê»˜ ì‚¬ìš© ë¶ˆê°€)')
    parser.add_argument('--layer-indices', type=int, nargs='+', default=None, 
                       help='ì¶”ì¶œí•  íŠ¹ì • ë ˆì´ì–´ ì¸ë±ìŠ¤ (ì˜ˆ: --layer-indices 2 3)')
    parser.add_argument('--start', type=int, default=0, help='ì‹œì‘ í”„ë ˆì„')
    parser.add_argument('--end', type=int, default=None, help='ì¢…ë£Œ í”„ë ˆì„')
    parser.add_argument('--output', type=str, default='vis_result', help='íŠ¹ì§• ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--device', type=str, default='cuda', choices=['cuda', 'cpu'], help='ë””ë°”ì´ìŠ¤')
    parser.add_argument('--demo', action='store_true', help='ë°ëª¨ ëª¨ë“œ (ë”ë¯¸ ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸)')
    parser.add_argument('--compare', action='store_true', help='ì—¬ëŸ¬ ëª¨ë¸ ë¹„êµ ëª¨ë“œ')
    parser.add_argument('--pca-dim', type=int, default=2, choices=[1, 2, 3], help='PCA ì¶•ì†Œ ì°¨ì› (1, 2, 3)')
    
    args = parser.parse_args()
    
    # layer-indicesì™€ layers ë™ì‹œ ì‚¬ìš© ì²´í¬
    if args.layer_indices is not None and args.layers != 4:
        print("âš ï¸  --layersì™€ --layer-indicesë¥¼ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   --layer-indicesë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ë ˆì´ì–´ ì„¤ì • ê²°ì •
    layer_config = args.layer_indices if args.layer_indices is not None else args.layers
    
    # ROIë¥¼ íŠœí”Œë¡œ ë³€í™˜
    roi_tuple = tuple(args.roi) if args.roi else None
    
    print("=== ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì§• ì¶”ì¶œ ===")
    if isinstance(layer_config, list):
        print(f"ğŸ“Œ ì¶”ì¶œí•  ë ˆì´ì–´: {layer_config}")
    else:
        print(f"ğŸ“Œ ì¶”ì¶œí•  ë ˆì´ì–´ ìˆ˜: {layer_config} (ë ˆì´ì–´ 0-{layer_config-1})")
    
    # ì „ì²˜ë¦¬ ëª¨ë“œ ì¶œë ¥
    if args.mask and roi_tuple:
        print(f"ğŸ“Œ ì „ì²˜ë¦¬ ëª¨ë“œ: ë§ˆìŠ¤í¬ + ROI")
    elif args.mask:
        print(f"ğŸ“Œ ì „ì²˜ë¦¬ ëª¨ë“œ: ë§ˆìŠ¤í¬ë§Œ")
    elif roi_tuple:
        print(f"ğŸ“Œ ì „ì²˜ë¦¬ ëª¨ë“œ: ROIë§Œ")
    else:
        print(f"ğŸ“Œ ì „ì²˜ë¦¬ ëª¨ë“œ: ì—†ìŒ (ì›ë³¸ í”„ë ˆì„)")
    print()
    
    device = args.device if torch.cuda.is_available() else 'cpu'
    if device == 'cpu' and args.device == 'cuda':
        print("âš ï¸  CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    try:
        features = extract_features_from_video(
            video_path=args.video,
            model_name=args.model,
            n_layers=layer_config,
            mask_path=args.mask,
            roi=roi_tuple,
            roi_format=args.roi_format,
            start_frame=args.start,
            end_frame=args.end,
            device=device,
            output_dir=args.output,
            pca_dim=args.pca_dim
        )
        
        print(f"\nğŸ“Š ì¶”ì¶œ ê²°ê³¼:")
        print(f"  ì´ í”„ë ˆì„ ìˆ˜: {len(features)}")
        print(f"  íŠ¹ì§• shape: {features[0].shape if features else 'N/A'}")
        
        if args.output:
            print(f"  ì €ì¥ ìœ„ì¹˜: {args.output}/")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
